{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !apt install python-opengl\n",
    "    !apt install ffmpeg\n",
    "    !apt install xvfb\n",
    "    !pip install pyvirtualdisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    \n",
    "    # Start virtual display\n",
    "    dis = Display(visible=0, size=(600, 400))\n",
    "    dis.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. A2C\n",
    "[Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016.](http://proceedings.mlr.press/v48/mniha16.pdf)\n",
    "\n",
    "### Actor-Critic\n",
    "\n",
    "*Actor critic* method is one of the popular *policy optimization* algorithms. This approach maximizes the expected return by pushing up the probabilities of actions that receive higher returns. Let $\\pi_\\theta$ denote a policy with parameters $\\theta$. The policy gradient of performance $\\mathcal{J}(\\pi_\\theta)$ is\n",
    "\n",
    "$$ \\nabla_\\theta \\mathcal{J}(\\pi_\\theta) = \\underset{\\tau\\sim\\pi_\\theta}{\\mathbb{E}}\\left[ \\sum^T_{t=0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)A^{\\pi_\\theta}(s_t, a_t) \\right],$$\n",
    "\n",
    "where $\\tau$ is a trajectory and $A^{\\pi_\\theta}$ is the advantage function for reducing variance of values. The *policy gradient algorithm* updates the parameters by adding this gradient.\n",
    "\n",
    "$$\\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta \\mathcal{J}(\\pi_{\\theta_k}),$$\n",
    "\n",
    "where $\\alpha$ is a learning rate. The agent is trained in an on-policy way because the parameters are updated by the current policy. We call the policy *Actor* which predicts probabilities of actions in each state, and call the value function *Critic* that predicts values of all state-action pairs. \n",
    "\n",
    "\n",
    "### Advantage Function\n",
    "\n",
    "The advantage function effectively reduces the variance of values and is defined as follows.\n",
    "\n",
    "$$ A(s,a) = Q(s,a) - V(s) $$\n",
    "\n",
    "From this formula, we can replace Q with $r+\\gamma V(s')$ and redefine Advantage function without using Action-Value function.\n",
    "\n",
    "$$ A(s,a) = r + \\gamma V(s') - V(s) $$\n",
    "\n",
    "\n",
    "### Maximization Entropy\n",
    "\n",
    "Entropy is a measure of unpredictability or a measure of randomness. If we have actions with almost equal probabilities, the entropy over the actions will be the largest because it's completely unpredictable which action will be chosen. In view of the fact, we can encourage exploration by adding entropy maximization term to the loss function. The entropy $H$ with respect to the probability $p$ over actions is\n",
    "\n",
    "$$ H(P) = - \\sum_a p(a) \\log p(a) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "import time\n",
    "from gym import wrappers\n",
    "# ROS packages required\n",
    "import rospy\n",
    "import rospkg\n",
    "import torch\n",
    "from openai_ros.openai_ros_common import StartOpenAI_ROS_Environment\n",
    "from gazebo_msgs.msg import ModelState\n",
    "from gazebo_msgs.srv import DeleteModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里请添加你自己的openAI_ros环境，默认是gym小游戏\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(2)\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.cudnn.enabled:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 777\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "We will use two separated networks for actor and critic respectively. The actor network consists of one fully connected hidden layer with ReLU branched out two fully connected output layers for mean and standard deviation of Normal distribution. Pendulum-v0 has only one action which has a range from -2 to 2. In order to fit the range, the actor outputs the mean value that is multiplied by 2 after tanh. On the one hand, the critic network has two fully connected layers as a hidden layer (ReLU) and an output layer. One thing to note is that we initialize the last layers' weights and biases as uniformly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uniformly(layer=nn.Linear, init_w=3e-3):\n",
    "    \"\"\"Initialize the weights and bias in [-init_w, init_w].\"\"\"\n",
    "    layer.weight.data.uniform_(-init_w, init_w)\n",
    "    layer.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, in_dim=int, out_dim=int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(in_dim, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.mu_layer = nn.Linear(128, out_dim)     \n",
    "        self.log_std_layer = nn.Linear(128, out_dim)   \n",
    "        \n",
    "        initialize_uniformly(self.mu_layer)\n",
    "        initialize_uniformly(self.log_std_layer)\n",
    "\n",
    "    def forward(self, state=torch.Tensor):\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_layer(x))\n",
    "        log_std = F.softplus(self.log_std_layer(x))\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return action, dist\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_dim=int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(in_dim, 128)\n",
    "        self.out = nn.Linear(128, 1)\n",
    "        \n",
    "        initialize_uniformly(self.out)\n",
    "\n",
    "    def forward(self, state=torch.Tensor):\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        value = self.out(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2C Agent\n",
    "Here is a summary of A2CAgent class.\n",
    "\n",
    "| Method           | Note                                                 |\n",
    "|---               |---                                                   |\n",
    "|select_action     | select an action from the input state.               |\n",
    "|step              | take an action and return the response of the env.   |\n",
    "|update_model      | update the model by gradient descent.                |\n",
    "|train             | train the agent during num_frames.                   |\n",
    "|test              | test the agent (1 episode).                          |\n",
    "|plot              | plot the training progresses.                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"A2CAgent interacting with environment.\n",
    "        \n",
    "    Atribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        gamma (float): discount factor\n",
    "        entropy_weight (float): rate of weighting entropy into the loss function\n",
    "        device (torch.device): cpu / gpu\n",
    "        actor (nn.Module): target actor model to select actions\n",
    "        critic (nn.Module): critic model to predict state values\n",
    "        actor_optimizer (optim.Optimizer) : optimizer of actor\n",
    "        critic_optimizer (optim.Optimizer) : optimizer of critic\n",
    "        transition (list): temporory storage for the recent transition\n",
    "        total_step (int): total step numbers\n",
    "        is_test (bool): flag to show the current mode (train / test)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env=gym.Env, gamma=float, entropy_weight=float):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.entropy_weight = entropy_weight\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "        \n",
    "        # networks\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
    "        self.critic = Critic(obs_dim).to(self.device)\n",
    "        \n",
    "        # optimizer\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "        # transition (state, log_prob, next_state, reward, done)\n",
    "        self.transition=list()\n",
    "        \n",
    "        # total steps count\n",
    "        self.total_step = 0\n",
    "\n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "        \n",
    "    def select_action(self, state=np.ndarray):\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action, dist = self.actor(state)\n",
    "        selected_action = dist.mean if self.is_test else action\n",
    "\n",
    "        if not self.is_test:\n",
    "            log_prob = dist.log_prob(selected_action).sum(dim=-1)\n",
    "            self.transition = [state, log_prob]\n",
    "        \n",
    "        return selected_action.clamp(-1.0, 1.0).cpu().detach().numpy()\n",
    "    \n",
    "    def step(self, action=np.ndarray):\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition.extend([next_state, reward, done])           \n",
    "    \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def update_model(self):\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"  \n",
    "        state, log_prob, next_state, reward, done = self.transition\n",
    "        # Q_t   = r + gamma * V(s_{t+1})  if state != Terminal\n",
    "        #       = r                       otherwise\n",
    "        mask = 1 - done\n",
    "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "        pred_value = self.critic(state)\n",
    "        targ_value = reward + self.gamma * self.critic(next_state) * mask\n",
    "        value_loss = F.smooth_l1_loss(pred_value, targ_value.detach())\n",
    "        \n",
    "        # update value\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # advantage = Q_t - V(s_t)\n",
    "        advantage = (targ_value - pred_value).detach()  # not backpropagated\n",
    "        policy_loss = -advantage * log_prob\n",
    "        policy_loss += self.entropy_weight * -log_prob  # entropy maximization\n",
    "\n",
    "        # update policy\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return policy_loss.item(), value_loss.item()\n",
    "    \n",
    "    def train(self, num_frames=int, plotting_interval = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        actor_losses, critic_losses, scores = [], [], []\n",
    "        state = self.env.reset()\n",
    "        score = 0\n",
    "        for i_episode in range(40000):\n",
    "            #state = env.reset()\n",
    "            #scores.append(score)\n",
    "            #score = 0\n",
    "            for self.total_step in range(1, num_frames + 1):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done = self.step(action)\n",
    "\n",
    "                actor_loss, critic_loss = self.update_model()#单步更新\n",
    "                #actor_losses.append(actor_loss)\n",
    "                #critic_losses.append(critic_loss)\n",
    "                #print('step:',env.env.cumulated_steps,'reward:',round(reward,2))\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                # if episode ends\n",
    "                if done:         \n",
    "                    #state = env.reset()\n",
    "                    #scores.append(score)\n",
    "                    print('i_episode:',i_episode,'score:',round(score,2))\n",
    "                    if i_episode%100==0:\n",
    "                        torch.save(self.actor, 'navigation_A2C_actor_128_hind_layers_1_episode_'+\n",
    "                                   str(i_episode)+'_score_'+str(score))\n",
    "                        torch.save(self.critic, 'navigation_A2C_critic_128_hind_layers_0_episode_'+\n",
    "                                   str(i_episode)+'_score_'+str(score))\n",
    "                    #score = 0\n",
    "                    break\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "                \n",
    "\n",
    "                # plot\n",
    "                #if self.total_step % plotting_interval == 0:\n",
    "                    #self._plot(self.total_step, scores, actor_losses, critic_losses)\n",
    "        #self.env.close()\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        frames = []\n",
    "        while not done:\n",
    "            frames.append(self.env.render(mode=\"rgb_array\"))\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx=int, \n",
    "        scores=List[float], \n",
    "        actor_losses=List[float], \n",
    "        critic_losses=List[float], \n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        def subplot(loc=int, title=str, values=List[float]):\n",
    "            plt.subplot(loc)\n",
    "            plt.title(title)\n",
    "            plt.plot(values)\n",
    "\n",
    "        subplot_params = [\n",
    "            (131, \"score\", scores),\n",
    "            (132, \"actor_loss\", actor_losses),\n",
    "            (133, \"critic_loss\", critic_losses),\n",
    "        ]\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(30, 5))\n",
    "        for loc, title, values in subplot_params:\n",
    "            subplot(loc, title, values)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see [the code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py) and [configurations](https://github.com/openai/gym/blob/cedecb35e3428985fd4efad738befeb75b9077f1/gym/envs/__init__.py#L81) of Pendulum-v0 from OpenAI's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNormalizer(gym.ActionWrapper):\n",
    "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
    "\n",
    "    def action(self, action=np.ndarray):\n",
    "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = action * scale_factor + reloc_factor\n",
    "        action = np.clip(action, low, high)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action=np.ndarray):\n",
    "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
    "        low = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        scale_factor = (high - low) / 2\n",
    "        reloc_factor = high - scale_factor\n",
    "\n",
    "        action = (action - reloc_factor) / scale_factor\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ActionNormalizer(env)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 2048\n",
    "gamma = 0.9\n",
    "entropy_weight = 1e-2\n",
    "\n",
    "agent = A2CAgent(env, gamma, entropy_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.train(num_frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
